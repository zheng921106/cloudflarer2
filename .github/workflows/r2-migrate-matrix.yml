name: Cloudflare R2 Bucket Migration (rclone) - Matrix Sharded

on:
  workflow_dispatch:
    inputs:
      SRC_BUCKET:
        description: "A桶名称（来源）"
        required: true
        default: "yasyadong001"
        type: string
      DST_BUCKET:
        description: "B桶名称（目标）"
        required: true
        default: "yas009"
        type: string
      MODE:
        description: "迁移模式"
        required: true
        default: "copy"
        type: choice
        options: ["copy", "sync"]
      TRANSFERS:
        description: "并发传输数"
        required: true
        default: "64"
        type: string
      CHECKERS:
        description: "并发校验数"
        required: true
        default: "128"
        type: string
      CHUNK_SIZE:
        description: "S3分块大小 (建议: 128M~512M)"
        required: true
        default: "256M"
        type: string
      SHARDS:
        description: "计划分片数（越大越安全，单片更短）"
        required: true
        default: "200"
        type: string
      MAX_PARALLEL:
        description: "矩阵最大并发（防止超配或被限速）"
        required: true
        default: "8"
        type: string

jobs:
  plan:
    name: Build object list & shard plan
    runs-on: ubuntu-latest
    timeout-minutes: 180   # ↑ 提高到3小时，避免大桶列举时超时
    outputs:
      matrix: ${{ steps.make-matrix.outputs.matrix }}
      shard_count: ${{ steps.make-matrix.outputs.count }}
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      SHARDS: ${{ inputs.SHARDS }}
      CHUNK_SIZE: ${{ inputs.CHUNK_SIZE }}
    steps:
      - name: Mask secrets
        run: |
          echo "::add-mask::${{ secrets.R2PUSH_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_ENDPOINT }}"

      - name: Install rclone & jq
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq
          rclone version

      - name: Prepare rclone.conf (source only for listing)
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}

      - name: Generate full object list (recursive)
        run: |
          set -e
          mkdir -p /tmp/plan
          # 断点友好：若上次中断，这里直接复用已生成的清单
          if [ -s /tmp/plan/all_keys.txt ]; then
            echo "Reuse existing /tmp/plan/all_keys.txt"
          else
            # 仅列出文件（不含“目录”占位），递归输出相对路径
            # --fast-list 对大量对象更高效
            rclone lsf r2src:"${SRC_BUCKET}" -R --files-only --fast-list > /tmp/plan/all_keys.txt
          fi
          echo "Total objects: $(wc -l < /tmp/plan/all_keys.txt)"

      - name: Split into shards
        id: split
        run: |
          set -e
          mkdir -p /tmp/plan/keylists
          # 按行均匀切分；对象很少时会产生更少分片
          split -n l/${SHARDS} -d -a 4 /tmp/plan/all_keys.txt /tmp/plan/keylists/chunk_
          COUNT=$(ls -1 /tmp/plan/keylists/chunk_* 2>/dev/null | wc -l || echo 0)
          echo "count=$COUNT" >> $GITHUB_OUTPUT
          echo "Shard files: $COUNT"

      - name: Build dynamic matrix JSON
        id: make-matrix
        run: |
          set -e
          cd /tmp/plan/keylists
          files=($(ls -1 chunk_* 2>/dev/null | sort || true))
          count=${#files[@]}
          if [ "$count" -eq 0 ]; then
            echo 'matrix={"chunk":[]}' >> $GITHUB_OUTPUT
            echo "count=0" >> $GITHUB_OUTPUT
            exit 0
          fi
          jq -n --argjson arr "$(printf '%s\n' "${files[@]}" | jq -R . | jq -s .)" '{chunk:$arr}' > /tmp/matrix.json
          echo "matrix=$(cat /tmp/matrix.json)" >> $GITHUB_OUTPUT
          echo "count=$count" >> $GITHUB_OUTPUT
          cat /tmp/matrix.json

      - name: Upload shard lists
        uses: actions/upload-artifact@v4
        with:
          name: r2-keylists
          path: /tmp/plan/keylists
          retention-days: 14

      - name: Summary
        run: |
          {
            echo "### 规划完成"
            echo "- 对象清单条目数：$(wc -l < /tmp/plan/all_keys.txt)"
            echo "- 切分份数：${{ steps.make-matrix.outputs.count }}"
          } >> $GITHUB_STEP_SUMMARY

  migrate:
    name: Migrate shard (${{ matrix.chunk }})
    needs: [plan]
    if: ${{ fromJson(needs.plan.outputs.matrix).chunk && fromJson(needs.plan.outputs.matrix).chunk.length > 0 }}
    runs-on: ubuntu-latest
    timeout-minutes: 350   # < 6小时，避免被系统强杀
    strategy:
      fail-fast: false
      max-parallel: ${{ inputs.MAX_PARALLEL }}
      matrix: ${{ fromJson(needs.plan.outputs.matrix) }}
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
      MODE: ${{ inputs.MODE }}
      TRANSFERS: ${{ inputs.TRANSFERS }}
      CHECKERS: ${{ inputs.CHECKERS }}
      CHUNK_SIZE: ${{ inputs.CHUNK_SIZE }}
    steps:
      - name: Mask secrets
        run: |
          echo "::add-mask::${{ secrets.R2PUSH_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_ENDPOINT }}"
          echo "::add-mask::${{ secrets.R2DST_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_ENDPOINT }}"

      - name: Install rclone & jq
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq
          rclone version

      - name: Prepare rclone.conf (src & dst)
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT: ${{ secrets.R2DST_ENDPOINT }}

      - name: Download shard list
        uses: actions/download-artifact@v4
        with:
          name: r2-keylists
          path: /tmp/keylists

      - name: Create migrate script (per-shard)
        run: |
          set -e
          cat > /tmp/migrate_one.sh <<'EOS'
          #!/usr/bin/env bash
          set -euo pipefail
          SHARD_FILE="$1"

          # 断点续传要点：
          # 1) --files-from 只复制本分片中的对象（天然可重试）
          # 2) --ignore-existing 避免重复覆盖已到位对象（幂等）
          # 3) 合理的重试与超时
          COMMON_ARGS="--progress --log-level INFO --stats=60s --stats-one-line \
          --transfers=${TRANSFERS} --checkers=${CHECKERS} \
          --s3-upload-cutoff=${CHUNK_SIZE} --s3-chunk-size=${CHUNK_SIZE} \
          --retries=10 --retries-sleep=30s --low-level-retries=30 \
          --timeout=300s --contimeout=120s \
          --buffer-size=512M --use-mmap \
          --multi-thread-streams=8 --multi-thread-cutoff=64M \
          --fast-list --s3-no-check-bucket \
          --ignore-existing"

          echo "== 开始迁移分片: $(basename "$SHARD_FILE") =="
          LOG="/tmp/${MODE}-$(basename "$SHARD_FILE").log"

          # 分片层级不要直接 sync（会误删分片外对象），统一使用 copy。
          if [[ "${MODE}" == "sync" ]]; then
            echo "[提示] MODE=sync 但在分片内先执行 copy；最终清理多余对象由 clean-extra 统一完成。"
          fi

          set +e
          rclone copy r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" \
            ${COMMON_ARGS} --files-from "$SHARD_FILE" 2>&1 | tee "$LOG"
          RC=${PIPESTATUS[0]}
          set -e

          echo "分片完成，退出码: $RC"
          exit $RC
          EOS
          chmod +x /tmp/migrate_one.sh

      - name: Run shard migration
        run: |
          set -e
          SHARD=/tmp/keylists/${{ matrix.chunk }}
          test -s "$SHARD" || { echo "空分片，跳过"; exit 0; }
          /tmp/migrate_one.sh "$SHARD"

      - name: Upload shard log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ matrix.chunk }}
          path: /tmp/*.log
          retention-days: 14

  verify:
    name: Final verify & report
    needs: [plan, migrate]
    if: ${{ always() }}
    runs-on: ubuntu-latest
    timeout-minutes: 180
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
      CHUNK_SIZE: ${{ inputs.CHUNK_SIZE }}
    steps:
      - name: Install rclone & jq
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq
          rclone version

      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT: ${{ secrets.R2DST_ENDPOINT }}

      - name: Quick size stats
        run: |
          {
            echo "### 统计信息"
            echo "\`\`\`json"
            echo "源桶："
            rclone size r2src:"${SRC_BUCKET}" --json | jq .
            echo ""
            echo "目标桶："
            rclone size r2dst:"${DST_BUCKET}" --json | jq .
            echo "\`\`\`"
          } >> $GITHUB_STEP_SUMMARY

      - name: Full bucket check (size-only)
        run: |
          set -e
          # R2 多段上传导致 ETag/MD5 不稳定，采用 size-only 快速校验
          rclone check r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" \
            --size-only --one-way --fast-list --checkers 256 \
            --log-file /tmp/check.log --log-level INFO || true

      - name: Random spot-check (20 files)
        run: |
          set -e
          tmp=$(mktemp)
          rclone lsf r2src:"${SRC_BUCKET}" -R --files-only --fast-list | shuf -n 20 > "$tmp"
          echo "随机抽查以下文件："
          cat "$tmp"
          echo ""
          while read -r f; do
            s1=$(rclone size "r2src:${SRC_BUCKET}/${f}" --json 2>/dev/null | jq -r '.bytes // -1')
            s2=$(rclone size "r2dst:${DST_BUCKET}/${f}" --json 2>/dev/null | jq -r '.bytes // -1')
            if [[ "$s1" -eq "$s2" && "$s1" -ge 0 ]]; then
              echo "✅ $f OK ($s1 bytes)"
            else
              echo "❌ $f mismatch: src=$s1 dst=$s2"
            fi
          done < "$tmp"

      - name: Upload verify logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: verify-logs
          path: /tmp/check.log
          retention-days: 14

  # （可选）如需“镜像”语义（删除目标多余对象），在完成所有 migrate + verify 后启用此 Job。
  clean-extra:
    if: ${{ github.event.inputs.MODE == 'sync' }}
    name: Clean extras on destination (sync after copy)
    needs: [verify]
    runs-on: ubuntu-latest
    timeout-minutes: 360
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
    steps:
      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version
      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT: ${{ secrets.R2DST_ENDPOINT }}

      - name: Prune extra objects (one-way sync delete)
        run: |
          # 注意：此步会删除 r2dst 中、但不在 r2src 的对象
          rclone sync r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" \
            --fast-list --size-only --delete-after --checkers 256 --transfers 64 \
            --log-level INFO --progress
