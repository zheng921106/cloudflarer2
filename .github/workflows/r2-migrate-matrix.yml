name: Cloudflare R2 Bucket Migration (rclone) - Matrix Sharded

on:
  workflow_dispatch:
    inputs:
      SRC_BUCKET:
        description: "A桶名称（来源）"
        required: true
        default: "yasyadong001"
      DST_BUCKET:
        description: "B桶名称（目标）"
        required: true
        default: "yas009"
      MODE:
        description: "迁移模式"
        required: true
        default: "copy"
        type: choice
        options: ["copy", "sync"]
      TRANSFERS:
        description: "并发传输数"
        required: true
        default: "64"
      CHECKERS:
        description: "并发校验数"
        required: true
        default: "128"
      CHUNK_SIZE:
        description: "S3分块大小 (建议: 128M~512M)"
        required: true
        default: "256M"
      SHARDS:
        description: "计划分片数（越大越安全，单片更短）"
        required: true
        default: "200"
      MAX_PARALLEL:
        description: "矩阵最大并发（防止超配或被限速）"
        required: true
        default: "8"

jobs:
  plan:
    name: Build object list & shard plan
    runs-on: ubuntu-latest
    timeout-minutes: 60
    outputs:
      matrix: ${{ steps.make-matrix.outputs.matrix }}
      shard_count: ${{ steps.make-matrix.outputs.count }}
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      SHARDS: ${{ inputs.SHARDS }}
      CHUNK_SIZE: ${{ inputs.CHUNK_SIZE }}
    steps:
      - name: Install rclone & jq
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq
          rclone version

      - name: Prepare rclone.conf (source only for listing)
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          force_path_style = true
          acl = private
          chunk_size = ${CHUNK_SIZE}
          upload_cutoff = ${CHUNK_SIZE}
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}

      - name: Generate full object list (recursive)
        run: |
          set -e
          mkdir -p /tmp/plan
          # 仅列出文件（不含“目录”占位），递归输出相对路径
          # 若对象极多，这一步是一次性成本；失败可重试
          rclone lsf r2src:"${SRC_BUCKET}" -R --files-only > /tmp/plan/all_keys.txt
          wc -l /tmp/plan/all_keys.txt || true

      - name: Split into shards
        id: split
        run: |
          set -e
          mkdir -p /tmp/plan/keylists
          # 均分为 ${SHARDS} 份；如对象很少，也会产生较少的片
          split -n l/${SHARDS} -d -a 4 /tmp/plan/all_keys.txt /tmp/plan/keylists/chunk_
          ls -l /tmp/plan/keylists | wc -l

      - name: Build dynamic matrix JSON
        id: make-matrix
        run: |
          set -e
          cd /tmp/plan/keylists
          files=($(ls -1 chunk_* | sort))
          count=${#files[@]}
          # 生成形如：{"chunk":["chunk_0000","chunk_0001",...]}
          jq -n --argjson arr "$(printf '%s\n' "${files[@]}" | jq -R . | jq -s .)" '{chunk:$arr}' > /tmp/matrix.json
          echo "matrix=$(cat /tmp/matrix.json)" >> $GITHUB_OUTPUT
          echo "count=$count" >> $GITHUB_OUTPUT
          cat /tmp/matrix.json

      - name: Upload shard lists
        uses: actions/upload-artifact@v4
        with:
          name: r2-keylists
          path: /tmp/plan/keylists
          retention-days: 7

      - name: Summary
        run: |
          {
            echo "### 规划完成"
            echo "- 对象清单条目数：$(wc -l < /tmp/plan/all_keys.txt)"
            echo "- 切分份数：${{ steps.make-matrix.outputs.count }}"
          } >> $GITHUB_STEP_SUMMARY

  migrate:
    name: Migrate shard (${{ matrix.chunk }})
    needs: [plan]
    runs-on: ubuntu-latest
    timeout-minutes: 360   # 每片<6小时；整体并行控制在 MAX_PARALLEL
    strategy:
      fail-fast: false
      max-parallel: ${{ inputs.MAX_PARALLEL }}
      matrix: ${{ fromJson(needs.plan.outputs.matrix) }}
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
      MODE: ${{ inputs.MODE }}
      TRANSFERS: ${{ inputs.TRANSFERS }}
      CHECKERS: ${{ inputs.CHECKERS }}
      CHUNK_SIZE: ${{ inputs.CHUNK_SIZE }}
    steps:
      - name: Install rclone & jq
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq
          rclone version

      - name: Prepare rclone.conf (src & dst)
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          force_path_style = true
          acl = private
          chunk_size = ${CHUNK_SIZE}
          upload_cutoff = ${CHUNK_SIZE}
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          force_path_style = true
          acl = private
          chunk_size = ${CHUNK_SIZE}
          upload_cutoff = ${CHUNK_SIZE}
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT: ${{ secrets.R2DST_ENDPOINT }}

      - name: Download shard list
        uses: actions/download-artifact@v4
        with:
          name: r2-keylists
          path: /tmp/keylists

      - name: Create migrate script (per-shard)
        run: |
          set -e
          cat > /tmp/migrate_one.sh <<'EOS'
          #!/usr/bin/env bash
          set -euo pipefail
          SHARD_FILE="$1"

          COMMON_ARGS="--progress --log-level INFO --stats=60s --stats-one-line \
          --transfers=${TRANSFERS} --checkers=${CHECKERS} \
          --s3-upload-cutoff=${CHUNK_SIZE} --s3-chunk-size=${CHUNK_SIZE} \
          --retries=10 --retries-sleep=30s --low-level-retries=20 \
          --timeout=300s --contimeout=120s \
          --buffer-size=512M --use-mmap \
          --multi-thread-streams=8 --multi-thread-cutoff=64M \
          --fast-list --no-traverse --s3-disable-checksum \
          --ignore-existing"   # 断点续传友好：已存在的对象直接跳过

          echo "== 开始迁移分片: $(basename "$SHARD_FILE") =="
          echo "时间: $(date)"
          echo "系统: $(uname -a)"
          echo "并发: transfers=${TRANSFERS}, checkers=${CHECKERS}, chunk=${CHUNK_SIZE}"

          # 使用 --files-from 精准迁移本分片对象集合
          LOG="/tmp/${MODE}-$(basename "$SHARD_FILE").log"

          if [[ "${MODE}" == "sync" ]]; then
            # sync 语义在分片层面会把“分片外”的对象忽略删除，所以不能直接 sync。
            # 因此对 sync 模式：执行 copy 分片迁移 + 统一 verify 后，再单独提供一个“清理多余对象”的可选 job（见下文 clean-extra）。
            echo "[提示] sync 在分片级不安全，本步骤按 copy 执行，后续通过 clean-extra 统一清理多余对象。"
          fi

          # 实施 copy
          set +e
          rclone copy r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" \
            ${COMMON_ARGS} --files-from "$SHARD_FILE" 2>&1 | tee "$LOG"
          RC=${PIPESTATUS[0]}
          set -e

          echo "分片完成，退出码: $RC"
          exit $RC
          EOS
          chmod +x /tmp/migrate_one.sh

      - name: Run shard migration
        run: |
          set -e
          SHARD=/tmp/keylists/${{ matrix.chunk }}
          test -s "$SHARD" || { echo "空分片，跳过"; exit 0; }
          /tmp/migrate_one.sh "$SHARD"

      - name: Upload shard log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ matrix.chunk }}
          path: |
            /tmp/*.log
          retention-days: 7

  verify:
    name: Final verify & report
    needs: [plan, migrate]
    runs-on: ubuntu-latest
    timeout-minutes: 120
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
      CHUNK_SIZE: ${{ inputs.CHUNK_SIZE }}
    steps:
      - name: Install rclone & jq
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq
          rclone version

      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          force_path_style = true
          acl = private
          chunk_size = ${CHUNK_SIZE}
          upload_cutoff = ${CHUNK_SIZE}
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          force_path_style = true
          acl = private
          chunk_size = ${CHUNK_SIZE}
          upload_cutoff = ${CHUNK_SIZE}
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT: ${{ secrets.R2DST_ENDPOINT }}

      - name: Quick size stats
        run: |
          {
            echo "### 统计信息"
            echo "\`\`\`json"
            echo "源桶："
            rclone size r2src:"${SRC_BUCKET}" --json | jq .
            echo ""
            echo "目标桶："
            rclone size r2dst:"${DST_BUCKET}" --json | jq .
            echo "\`\`\`"
          } >> $GITHUB_STEP_SUMMARY

      - name: Full bucket check (size-only)
        run: |
          set -e
          # S3/R2 多段上传导致 ETag/MD5 不可靠，采用 --size-only 快速校验
          # 如需更强校验，可在后续使用 rclone md5sum 仅对小文件抽样
          rclone check r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" --size-only --one-way --fast-list --checkers 256 --log-file /tmp/check.log --log-level INFO || true

      - name: Random spot-check (20 files)
        run: |
          set -e
          tmp=$(mktemp)
          rclone lsf r2src:"${SRC_BUCKET}" -R --files-only | shuf -n 20 > "$tmp"
          echo "随机抽查以下文件："
          cat "$tmp"
          echo ""
          while read -r f; do
            s1=$(rclone size "r2src:${SRC_BUCKET}/${f}" --json 2>/dev/null | jq -r '.bytes // -1')
            s2=$(rclone size "r2dst:${DST_BUCKET}/${f}" --json 2>/dev/null | jq -r '.bytes // -1')
            if [[ "$s1" -eq "$s2" && "$s1" -ge 0 ]]; then
              echo "✅ $f OK ($s1 bytes)"
            else
              echo "❌ $f mismatch: src=$s1 dst=$s2"
            fi
          done < "$tmp"

      - name: Upload verify logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: verify-logs
          path: |
            /tmp/check.log
          retention-days: 7

  # （可选）若你一定要“同步”语义（删除目标多余对象），单独运行此 Job：
  # 1) 先完成所有 migrate 分片 + verify
  # 2) 再触发 clean-extra，它会在目标侧删除源中不存在的对象
  clean-extra:
    if: ${{ github.event.inputs.MODE == 'sync' }}
    name: Clean extras on destination (sync after copy)
    needs: [verify]
    runs-on: ubuntu-latest
    timeout-minutes: 360
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
      CHUNK_SIZE: ${{ inputs.CHUNK_SIZE }}
    steps:
      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version
      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          force_path_style = true
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT: ${{ secrets.R2DST_ENDPOINT }}
      - name: Prune extra objects (one-way sync delete)
        run: |
          # 注意：此步会删除 r2dst 中、但不在 r2src 的对象
          rclone sync r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" \
            --fast-list --size-only --delete-after --checkers 256 --transfers 64 \
            --log-level INFO --progress
