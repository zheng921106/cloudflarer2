name: Cloudflare R2 Bucket Migration (rclone) - Simple & Robust

on:
  workflow_dispatch:
    inputs:
      SRC_BUCKET:
        description: "A桶（来源）"
        required: true
        default: "yasyadong001"
        type: string
      DST_BUCKET:
        description: "B桶（目标）"
        required: true
        default: "yas012"
        type: string
      MODE:
        description: "迁移模式"
        required: true
        default: "copy"
        type: choice
        options: ["copy", "sync"]   # sync 只在最后一步真正删除多余对象
      TRANSFERS:
        description: "rclone 并发传输数"
        required: true
        default: "64"
        type: string
      CHECKERS:
        description: "rclone 并发校验数"
        required: true
        default: "128"
        type: string
      CHUNK_SIZE:
        description: "S3分块大小 (128M~512M)"
        required: true
        default: "256M"
        type: string
      SHARDS:
        description: "逻辑分片数（并行度上限）"
        required: true
        default: "200"
        type: string
      MAX_PARALLEL:
        description: "同时处理的分片数量（<= GitHub 允许并发）"
        required: true
        default: "8"
        type: string

jobs:
  migrate:
    name: Migrate ${ { github.event.inputs.SRC_BUCKET } } -> ${ { github.event.inputs.DST_BUCKET } }
    runs-on: ubuntu-latest
    timeout-minutes: 420  # 7小时，避免大桶被强杀
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
      MODE:       ${{ inputs.MODE }}
      TRANSFERS:  ${{ inputs.TRANSFERS }}
      CHECKERS:   ${{ inputs.CHECKERS }}
      CHUNK_SIZE: ${{ inputs.CHUNK_SIZE }}
      SHARDS:     ${{ inputs.SHARDS }}
      MAX_PAR:    ${{ inputs.MAX_PARALLEL }}
    steps:
      - name: Mask secrets
        run: |
          echo "::add-mask::${{ secrets.R2PUSH_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_ENDPOINT }}"
          echo "::add-mask::${{ secrets.R2DST_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_ENDPOINT }}"

      - name: Install rclone, jq, GNU parallel
        run: |
          set -euxo pipefail
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq parallel
          rclone version
          parallel --version

      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT:  ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT:   ${{ secrets.R2DST_ENDPOINT }}

      - name: Preflight checks (端点/权限/桶存在性)
        run: |
          set -euxo pipefail
          # 端点必须以 https:// 起，且包含 account_id.r2.cloudflarestorage.com
          for v in "${{ secrets.R2PUSH_ENDPOINT }}" "${{ secrets.R2DST_ENDPOINT }}"; do
            echo "$v" | grep -E '^https://[a-z0-9]{32}\.r2\.cloudflarestorage\.com$' >/dev/null || {
              echo "::error::R2 endpoint 不符合预期格式：$v"
              exit 1
            }
          done
          # 桶可列举
          rclone lsd "r2src:${SRC_BUCKET}" || { echo "::error::无法访问源桶 ${SRC_BUCKET}"; exit 1; }
          # 目标桶不存在时，R2 默认不能自动创建；提前尝试创建（忽略失败）
          rclone mkdir "r2dst:${DST_BUCKET}" || true
          rclone lsd "r2dst:${DST_BUCKET}" || { echo "::error::无法访问目标桶 ${DST_BUCKET}"; exit 1; }
          echo "✅ Preflight OK"

      - name: Build object list & shard (本地临时目录)
        run: |
          set -euxo pipefail
          mkdir -p /tmp/plan/keylists
          # 断点友好：存在则复用
          if [ -s /tmp/plan/all_keys.txt ]; then
            echo "Reuse /tmp/plan/all_keys.txt"
          else
            # 只列出文件（相对路径），提高性能使用 --fast-list
            rclone lsf "r2src:${SRC_BUCKET}" -R --files-only --fast-list > /tmp/plan/all_keys.txt
          fi
          echo "Total objects: $(wc -l < /tmp/plan/all_keys.txt)"

          # 空桶保护
          if [ "$(wc -l < /tmp/plan/all_keys.txt)" -eq 0 ]; then
            echo "::warning::源桶为空，无需迁移。"
            exit 0
          fi

          # 均匀切片
          split -n l/${SHARDS} -d -a 4 /tmp/plan/all_keys.txt /tmp/plan/keylists/chunk_
          ls -1 /tmp/plan/keylists/chunk_* | wc -l

      - name: Create migrate script
        run: |
          cat > /tmp/migrate_one.sh <<'EOS'
          #!/usr/bin/env bash
          set -euo pipefail
          SHARD="$1"
          echo "== 开始分片: $(basename "$SHARD") =="

          COMMON_ARGS="--progress --log-level INFO --stats=60s --stats-one-line \
            --transfers=${TRANSFERS} --checkers=${CHECKERS} \
            --s3-upload-cutoff=${CHUNK_SIZE} --s3-chunk-size=${CHUNK_SIZE} \
            --retries=10 --retries-sleep=30s --low-level-retries=30 \
            --timeout=300s --contimeout=120s \
            --buffer-size=512M --use-mmap \
            --multi-thread-streams=8 --multi-thread-cutoff=64M \
            --fast-list --s3-no-check-bucket \
            --ignore-existing"

          # 重要：.m3u8 / .shtml 的 Content-Type
          # rclone 默认按扩展名推断；确保文件名含扩展名即可。如果你的源端缺少扩展名，可考虑单独二次修正（后文给修复脚本）。

          # 分片内一律 copy（避免 sync 误删分片外对象）
          set +e
          rclone copy "r2src:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            ${COMMON_ARGS} --files-from "$SHARD" 2>&1 | tee "/tmp/$(basename "$SHARD").log"
          RC=${PIPESTATUS[0]}
          set -e
          exit $RC
          EOS
          chmod +x /tmp/migrate_one.sh

      - name: Run shards in parallel (受控并发)
        run: |
          set -euxo pipefail
          ls -1 /tmp/plan/keylists/chunk_* > /tmp/shards.list
          # 用 GNU parallel 控制并行度，失败会在最后聚合返回码
          parallel -j "${MAX_PAR}" --halt now,fail=1 /tmp/migrate_one.sh {} :::: /tmp/shards.list

      - name: Quick size stats
        if: always()
        run: |
          {
            echo "### 统计信息"
            echo "\`\`\`json"
            echo "源桶："
            rclone size "r2src:${SRC_BUCKET}" --json | jq .
            echo ""
            echo "目标桶："
            rclone size "r2dst:${DST_BUCKET}" --json | jq .
            echo "\`\`\`"
          } >> $GITHUB_STEP_SUMMARY

      - name: Full bucket check (size-only)
        if: always()
        run: |
          set -euxo pipefail
          # R2 多段上传导致 ETag/MD5 不稳定，采用 size-only 快速校验
          rclone check "r2src:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            --size-only --one-way --fast-list --checkers 256 \
            --log-file /tmp/check.log --log-level INFO || true

      - name: Random spot-check (20 files)
        if: always()
        run: |
          set -euxo pipefail
          tmp=$(mktemp)
          rclone lsf "r2src:${SRC_BUCKET}" -R --files-only --fast-list | shuf -n 20 > "$tmp"
          echo "随机抽查以下文件：" >> $GITHUB_STEP_SUMMARY
          sed 's/^/- /' "$tmp" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          while read -r f; do
            s1=$(rclone size "r2src:${SRC_BUCKET}/${f}" --json 2>/dev/null | jq -r '.bytes // -1')
            s2=$(rclone size "r2dst:${DST_BUCKET}/${f}" --json 2>/dev/null | jq -r '.bytes // -1')
            if [[ "$s1" -eq "$s2" && "$s1" -ge 0 ]]; then
              echo "✅ $f OK ($s1 bytes)"
            else
              echo "❌ $f mismatch: src=$s1 dst=$s2"
            fi
          done < "$tmp"

      - name: Upload logs (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: r2-migrate-logs
          path: /tmp/*.log
          retention-days: 7

      - name: Final prune extras when MODE=sync
        if: ${{ inputs.MODE == 'sync' }}
        run: |
          set -euxo pipefail
          # 只在所有 copy 完成后做一次镜像删除（更安全）
          rclone sync "r2src:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            --fast-list --size-only --delete-after --checkers 256 --transfers 64 \
            --log-level INFO --progress
