name: Cloudflare R2 Bucket Migration (rclone) - Robust Version

on:
  workflow_dispatch:
    inputs:
      SRC_BUCKET:
        description: "A桶名称（来源桶名）"
        required: true
        type: string
        default: "yasyadong001"
      DST_BUCKET:
        description: "B桶名称（目标桶名）"
        required: true
        type: string
        default: "yas007"
      MODE:
        description: "迁移模式：copy(仅新增/更新) 或 sync(镜像，会删除目标多余对象)"
        required: true
        default: "copy"
        type: choice
        options: ["copy", "sync"]
      TRANSFERS:
        description: "并发传输数 (--transfers)"
        required: true
        default: "16"  # 降低并发以减少压力
        type: string
      CHECKERS:
        description: "并发校验数 (--checkers)"
        required: true
        default: "32"  # 降低校验并发
        type: string
      S3_CHUNK_SIZE:
        description: "S3分片大小 (--s3-chunk-size)"
        required: true
        default: "64M"  # 减小分片大小
        type: string
      S3_UPLOAD_CUTOFF:
        description: "直传阈值 (--s3-upload-cutoff)"
        required: true
        default: "64M"  # 减小直传阈值
        type: string
      BATCH_SIZE:
        description: "分批处理大小（GB），0表示不分批"
        required: true
        default: "500"
        type: string

jobs:
  migrate:
    name: Migrate A→B with rclone (Robust)
    runs-on: ubuntu-latest
    timeout-minutes: 4320  # 3天超时，适应15TB数据
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
      MODE: ${{ inputs.MODE }}
      TRANSFERS: ${{ inputs.TRANSFERS }}
      CHECKERS: ${{ inputs.CHECKERS }}
      S3_CHUNK_SIZE: ${{ inputs.S3_CHUNK_SIZE }}
      S3_UPLOAD_CUTOFF: ${{ inputs.S3_UPLOAD_CUTOFF }}
      BATCH_SIZE: ${{ inputs.BATCH_SIZE }}

    steps:
      - name: Install rclone
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Prepare rclone config
        run: |
          set -e
          mkdir -p ~/.config/rclone

          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          force_path_style = true
          acl = private
          # 增加连接参数
          disable_checksum = false
          memory_pool_flush_time = 30s
          memory_pool_use_mmap = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          force_path_style = true
          acl = private
          # 增加连接参数
          disable_checksum = false
          memory_pool_flush_time = 30s
          memory_pool_use_mmap = true
          EOF

          for v in R2PUSH_ACCESS_KEY R2PUSH_SECRET_KEY R2PUSH_ENDPOINT R2DST_ACCESS_KEY R2DST_SECRET_KEY R2DST_ENDPOINT; do
            if [ -z "${!v}" ]; then
              echo "❌ 缺少必填 Secret: $v" >&2
              exit 11
            fi
          done
          case "${R2PUSH_ENDPOINT}" in https://*) ;; *) echo "❌ R2PUSH_ENDPOINT 必须以 https:// 开头"; exit 12;; esac
          case "${R2DST_ENDPOINT}"  in https://*) ;; *) echo "❌ R2DST_ENDPOINT 必须以 https:// 开头";  exit 13;; esac

          echo "✅ rclone.conf 已生成"
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT: ${{ secrets.R2DST_ENDPOINT }}

      - name: Preflight checks
        run: |
          set -e
          echo "== 检查源桶可访问性 =="
          rclone ls r2src:"${SRC_BUCKET}" --max-depth 0 >/dev/null || (echo "❌ 源桶不可访问：${SRC_BUCKET}" >&2; exit 21)
          
          echo "== 检查目标桶可访问性 =="
          rclone ls r2dst:"${DST_BUCKET}" --max-depth 0 >/dev/null || (echo "❌ 目标桶不可访问：${DST_BUCKET}" >&2; exit 22)
          
          echo "== 检查目标桶写权限 =="
          echo "test" > /tmp/.r2_write_probe.txt
          if ! rclone copyto /tmp/.r2_write_probe.txt r2dst:"${DST_BUCKET}"/.r2_write_probe.txt; then
            echo "❌ 无法写入 ${DST_BUCKET}" >&2
            exit 23
          fi
          rclone deletefile r2dst:"${DST_BUCKET}"/.r2_write_probe.txt || true
          echo "✅ 权限检查通过"

      - name: Estimate migration size
        run: |
          set -e
          echo "== 估算源桶大小 =="
          rclone size r2src:"${SRC_BUCKET}" --json | jq . || true
          echo "== 列出前100个文件作为样本 =="
          rclone lsf r2src:"${SRC_BUCKET}" -R --files-only | head -n 100 || true

      - name: Run robust migration
        run: |
          set -e
          # 核心迁移参数 - 针对大文件和稳定性优化
          COMMON_ARGS="--progress --log-level INFO --stats=60s --stats-one-line \
          --transfers=${TRANSFERS} --checkers=${CHECKERS} \
          --s3-upload-cutoff=${S3_UPLOAD_CUTOFF} --s3-chunk-size=${S3_CHUNK_SIZE} \
          --metadata --checksum --size-only \
          --retries=10 --retries-sleep=30s --low-level-retries=30 \
          --timeout=300s --contimeout=120s \
          --buffer-size=256M --use-mmap \
          --multi-thread-streams=4 --multi-thread-cutoff=32M \
          --fast-list --no-traverse"

          # 分批处理逻辑（如果启用）
          BATCH_GB=${BATCH_SIZE}
          if [ "$BATCH_GB" -gt "0" ]; then
            echo "⚠️  分批处理已启用：每 ${BATCH_GB}GB 进行一次提交"
            COMMON_ARGS="$COMMON_ARGS --max-transfer=${BATCH_GB}G"
          fi

          echo "== 开始迁移 =="
          echo "使用的参数: $COMMON_ARGS"

          if [[ "${MODE}" == "sync" ]]; then
            echo ">> SYNC 模式：目标将与来源完全一致"
            rclone sync r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" $COMMON_ARGS --delete-after
          else
            echo ">> COPY 模式：仅新增/更新"
            rclone copy r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" $COMMON_ARGS
          fi

          echo "✅ 迁移完成"

      - name: Verify migration integrity
        run: |
          set -e
          echo "== 迁移后完整性检查 =="
          
          # 检查文件数量
          echo "源桶文件数量:"
          rclone ls r2src:"${SRC_BUCKET}" | wc -l || true
          echo "目标桶文件数量:"
          rclone ls r2dst:"${DST_BUCKET}" | wc -l || true
          
          # 检查总大小
          echo "源桶总大小:"
          rclone size r2src:"${SRC_BUCKET}" --json | jq .bytes || true
          echo "目标桶总大小:"
          rclone size r2dst:"${DST_BUCKET}" --json | jq .bytes || true
          
          # 抽样检查一些文件
          echo "随机抽样检查10个文件:"
          for i in $(seq 1 10); do
            file=$(rclone lsf r2src:"${SRC_BUCKET}" -R --files-only | shuf -n 1)
            if [ -n "$file" ]; then
              src_size=$(rclone size r2src:"${SRC_BUCKET}/$file" --json 2>/dev/null | jq .bytes || echo "N/A")
              dst_size=$(rclone size r2dst:"${DST_BUCKET}/$file" --json 2>/dev/null | jq .bytes || echo "N/A")
              echo "文件: $file | 源大小: $src_size | 目标大小: $dst_size | 状态: $([ "$src_size" = "$dst_size" ] && echo "✅" || echo "❌")"
            fi
          done

      - name: Generate summary report
        if: always()
        run: |
          {
            echo "### R2 Migration Summary Report"
            echo "- **模式**: \`${MODE}\`"
            echo "- **源桶**: \`${SRC_BUCKET}\`"
            echo "- **目标桶**: \`${DST_BUCKET}\`"
            echo "- **并发设置**: Transfers=\`${TRANSFERS}\`, Checkers=\`${CHECKERS}\`"
            echo "- **分片设置**: Chunk-size=\`${S3_CHUNK_SIZE}\`, Upload-cutoff=\`${S3_UPLOAD_CUTOFF}\`"
            echo "- **分批大小**: \`${BATCH_SIZE}GB\`"
            echo ""
            echo "#### 迁移后状态"
            echo "\`\`\`"
            echo "源桶统计:"
            rclone size r2src:"${SRC_BUCKET}" --json 2>/dev/null | jq || echo "无法获取源桶统计"
            echo ""
            echo "目标桶统计:"
            rclone size r2dst:"${DST_BUCKET}" --json 2>/dev/null | jq || echo "无法获取目标桶统计"
            echo "\`\`\`"
          } >> $GITHUB_STEP_SUMMARY
