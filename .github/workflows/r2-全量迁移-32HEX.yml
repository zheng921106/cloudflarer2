name: Cloudflare R2 Bucket Migration (rclone)

on:
  workflow_dispatch:
    inputs:
      SRC_BUCKET:
        description: "A桶名称（来源桶名）"
        required: true
        type: string
        default: "yasyadong001"
      DST_BUCKET:
        description: "B桶名称（目标桶名）"
        required: true
        type: string
        default: "yas007"  # ← 按你的实际目标桶修改
      MODE:
        description: "迁移模式：copy(仅新增/更新) 或 sync(镜像，会删除目标多余对象)"
        required: true
        default: "copy"
        type: choice
        options: ["copy", "sync"]
      TRANSFERS:
        description: "并发传输数 (--transfers)"
        required: true
        default: "32"
        type: string
      CHECKERS:
        description: "并发校验数 (--checkers)"
        required: true
        default: "64"
        type: string
      S3_CHUNK_SIZE:
        description: "S3分片大小 (--s3-chunk-size)"
        required: true
        default: "100M"
        type: string
      S3_UPLOAD_CUTOFF:
        description: "直传阈值 (--s3-upload-cutoff)"
        required: true
        default: "100M"
        type: string

jobs:
  migrate:
    name: Migrate A→B with rclone
    runs-on: ubuntu-latest
    timeout-minutes: 1440
    env:
      SRC_BUCKET: ${{ inputs.SRC_BUCKET }}
      DST_BUCKET: ${{ inputs.DST_BUCKET }}
      MODE: ${{ inputs.MODE }}
      TRANSFERS: ${{ inputs.TRANSFERS }}
      CHECKERS: ${{ inputs.CHECKERS }}
      S3_CHUNK_SIZE: ${{ inputs.S3_CHUNK_SIZE }}
      S3_UPLOAD_CUTOFF: ${{ inputs.S3_UPLOAD_CUTOFF }}

    steps:
      - name: Install rclone
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Prepare rclone config
        run: |
          set -e
          mkdir -p ~/.config/rclone

          # 注意：必须用 <<EOF（无引号）保证环境变量会被展开
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          force_path_style = true
          acl = private

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          force_path_style = true
          acl = private
          EOF

          # 基本校验：Secrets 是否注入、端点是否以 https:// 开头
          for v in R2PUSH_ACCESS_KEY R2PUSH_SECRET_KEY R2PUSH_ENDPOINT R2DST_ACCESS_KEY R2DST_SECRET_KEY R2DST_ENDPOINT; do
            if [ -z "${!v}" ]; then
              echo "❌ 缺少必填 Secret: $v" >&2
              exit 11
            fi
          done
          case "${R2PUSH_ENDPOINT}" in https://*) ;; *) echo "❌ R2PUSH_ENDPOINT 必须以 https:// 开头"; exit 12;; esac
          case "${R2DST_ENDPOINT}"  in https://*) ;; *) echo "❌ R2DST_ENDPOINT 必须以 https:// 开头";  exit 13;; esac

          echo "✅ rclone.conf 已生成"
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT: ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT: ${{ secrets.R2DST_ENDPOINT }}

      # 不使用 ListBuckets；直接对具体桶操作，兼容性更好
      - name: Preflight A — 源/目标桶可读性检查（按桶列根目录）
        run: |
          set -e
          echo "== Check source bucket exists & readable =="
          rclone ls r2src:"${SRC_BUCKET}" --max-depth 0 >/dev/null || (echo "❌ 源桶不可访问：${SRC_BUCKET}" >&2; exit 21)
          echo "== Check destination bucket exists & readable =="
          rclone ls r2dst:"${DST_BUCKET}" --max-depth 0 >/dev/null || (echo "❌ 目标桶不可访问：${DST_BUCKET}（请确认：已创建/Endpoint 正确/Token 有权限）" >&2; exit 22)
          echo "✅ 两个桶都可读"

      - name: Preflight B — 目标桶写权限探测
        run: |
          set -e
          echo "test" > /tmp/.r2_write_probe.txt
          if ! rclone copyto /tmp/.r2_write_probe.txt r2dst:"${DST_BUCKET}"/.r2_write_probe.txt; then
            echo "❌ 无法写入 ${DST_BUCKET}，请检查 R2DST_* Token 权限（至少 PutObject；若用 sync 还需 DeleteObject）" >&2
            exit 23
          fi
          rclone deletefile r2dst:"${DST_BUCKET}"/.r2_write_probe.txt || true
          echo "✅ 写权限正常"

      - name: Sanity check（递归列出前50个文件）
        run: |
          set -e
          echo "Source (first 50 files):"
          rclone lsf r2src:"${SRC_BUCKET}" -R --files-only | head -n 50 || true
          echo
          echo "Destination (first 50 files):"
          rclone lsf r2dst:"${DST_BUCKET}" -R --files-only | head -n 50 || true

      - name: Run migration（真实迁移；失败自动给出调试片段）
        run: |
          set -e
          COMMON_ARGS="-P --log-level NOTICE --stats=30s --stats-one-line \
          --transfers=${TRANSFERS} --checkers=${CHECKERS} \
          --s3-upload-cutoff=${S3_UPLOAD_CUTOFF} --s3-chunk-size=${S3_CHUNK_SIZE} \
          --metadata --retries=5 --low-level-retries=20 --retries-sleep=10s"

          if [[ "${MODE}" == "sync" ]]; then
            echo ">> SYNC 模式：目标将与来源完全一致（包含删除多余对象）"
            set +e
            rclone sync r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" $COMMON_ARGS --delete-after
            rc=$?
            set -e
          else
            echo ">> COPY 模式：仅新增/更新，不删除目标多余对象"
            set +e
            rclone copy r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" $COMMON_ARGS
            rc=$?
            set -e
          fi

          if [[ $rc -ne 0 ]]; then
            echo "❌ 迁移失败（rc=$rc）。输出最后200行 DEBUG 供排查："
            rclone copy r2src:"${SRC_BUCKET}" r2dst:"${DST_BUCKET}" -vv --dump headers --retries 1 --transfers=4 --checkers=8 --size-only --dry-run | tail -n 200 || true
            exit $rc
          fi

      - name: Compare sizes（迁移后大小对比）
        run: |
          set -e
          echo "Source size:"
          rclone size r2src:"${SRC_BUCKET}" || true
          echo "Destination size:"
          rclone size r2dst:"${DST_BUCKET}" || true

      - name: Emit summary
        if: always()
        run: |
          {
            echo "### R2 Migration Summary"
            echo "- Mode: \`${MODE}\`"
            echo "- Source bucket: \`${SRC_BUCKET}\`"
            echo "- Destination bucket: \`${DST_BUCKET}\`"
            echo "- Transfers: \`${TRANSFERS}\` | Checkers: \`${CHECKERS}\`"
            echo "- s3-chunk-size: \`${S3_CHUNK_SIZE}\` | s3-upload-cutoff: \`${S3_UPLOAD_CUTOFF}\`"
            echo ""
            echo "#### Destination sample (first 100 files)"
            echo "\`\`\`"
            rclone lsf r2dst:"${DST_BUCKET}" -R --files-only | head -n 100 || true
            echo "\`\`\`"
          } >> $GITHUB_STEP_SUMMARY
