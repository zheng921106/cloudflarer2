name: Cloudflare R2 Mega Migration (stream-shard + pass-chaining)

on:
  workflow_dispatch:
    inputs:
      SRC_BUCKET:   { description: "A桶（来源）", required: true, default: "yasyadong001", type: string }
      DST_BUCKET:   { description: "B桶（目标）", required: true, default: "yas012", type: string }
      MODE:         { description: "模式：copy(默认) / sync(最后做一次清理)", required: true, default: "copy", type: choice, options: ["copy","sync"] }
      SHARDS:       { description: "总分片数（建议 1024~4096）", required: true, default: "2048", type: string }
      PASS_INDEX:   { description: "当前波次（从0开始）", required: true, default: "0", type: string }
      PASS_COUNT:   { description: "总波次数（越大每波越短）", required: true, default: "12", type: string }
      MAX_PARALLEL: { description: "每波并行分片数（保守：4~8）", required: true, default: "6", type: string }
      TRANSFERS:    { description: "rclone传输并发（小文件建议 64~128）", required: true, default: "96", type: string }
      CHECKERS:     { description: "rclone校验并发", required: true, default: "256", type: string }
      CHUNK_SIZE:   { description: "S3分块阈值/大小（小文件别太低）", required: true, default: "256M", type: string }

jobs:
  migrate:
    name: PASS ${{ inputs.PASS_INDEX }}/${{ inputs.PASS_COUNT }} • ${{ inputs.SRC_BUCKET }} → ${{ inputs.DST_BUCKET }}
    runs-on: ubuntu-latest
    timeout-minutes: 350
    permissions:
      actions: write
    env:
      SRC_BUCKET:   ${{ inputs.SRC_BUCKET }}
      DST_BUCKET:   ${{ inputs.DST_BUCKET }}
      MODE:         ${{ inputs.MODE }}
      SHARDS:       ${{ inputs.SHARDS }}
      PASS_INDEX:   ${{ inputs.PASS_INDEX }}
      PASS_COUNT:   ${{ inputs.PASS_COUNT }}
      MAX_PAR:      ${{ inputs.MAX_PARALLEL }}
      TRANSFERS:    ${{ inputs.TRANSFERS }}
      CHECKERS:     ${{ inputs.CHECKERS }}
      CHUNK_SIZE:   ${{ inputs.CHUNK_SIZE }}

    steps:
      - name: Mask secrets
        run: |
          echo "::add-mask::${{ secrets.R2PUSH_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_ENDPOINT }}"
          echo "::add-mask::${{ secrets.R2DST_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_ENDPOINT }}"

      - name: Install rclone, jq, GNU parallel, mawk
        run: |
          set -euxo pipefail
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq parallel mawk
          rclone version
          parallel --version
          mawk -W version || true

      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT:  ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT:   ${{ secrets.R2DST_ENDPOINT }}

      - name: Preflight checks（端点/桶）
        run: |
          set -euxo pipefail
          rclone lsd "r2src:${SRC_BUCKET}"
          rclone mkdir "r2dst:${DST_BUCKET}" || true
          rclone lsd "r2dst:${DST_BUCKET}"

      - name: Stream-shard only shards for THIS PASS（无全量落地清单）
        run: |
          set -euxo pipefail
          mkdir -p /tmp/pass
          cat > /tmp/stream_shard.awk <<'AWK'
          BEGIN {
            pc = ENVIRON["PASS_COUNT"] + 0
            pi = ENVIRON["PASS_INDEX"] + 0
            shards = ENVIRON["SHARDS"] + 0
          }
          function djb2(s,   h,i,c) {
            h = 5381
            for (i=1; i<=length(s); i++) {
              c = ord(substr(s,i,1))
              h = ((h * 33) + c) % 2147483647
            }
            return h
          }
          function ord(ch) { return index("\0\1\2\3\4\5\6\7\10\11\12\13\14\15\16\17\20\21\22\23\24\25\26\27\30\31\32\33\34\35\36\37 !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~", ch)-1 }
          {
            key = $0
            h = djb2(key)
            sid = h % shards
            if ((sid % pc) == pi) {
              fn = sprintf("/tmp/pass/chunk_%04d", sid)
              print key >> fn
            }
          }
          AWK

          rclone lsjson "r2src:${SRC_BUCKET}" -R --files-only --no-mimetype --no-modtime \
            | jq -r '.[].Path' \
            | mawk -f /tmp/stream_shard.awk

          cnt=$(ls -1 /tmp/pass/chunk_* 2>/dev/null | wc -l || echo 0)
          echo "This PASS shards: ${cnt}"
          if [ "$cnt" -eq 0 ]; then
            echo "::notice::本 PASS 没有需要处理的分片。"
            exit 0
          fi

      - name: Create migrate script（小文件优化参数）
        run: |
          cat > /tmp/migrate_one.sh <<'EOS'
          #!/usr/bin/env bash
          set -euo pipefail
          SHARD="$1"
          COMMON_ARGS="--progress --log-level INFO --stats=60s --stats-one-line \
            --transfers=${TRANSFERS} --checkers=${CHECKERS} \
            --s3-upload-cutoff=${CHUNK_SIZE} --s3-chunk-size=${CHUNK_SIZE} \
            --s3-upload-concurrency=4 \
            --retries=20 --retries-sleep=20s --low-level-retries=60 \
            --timeout=300s --contimeout=120s \
            --buffer-size=32M --use-mmap \
            --multi-thread-streams=4 --multi-thread-cutoff=16M \
            --s3-no-check-bucket \
            --ignore-existing \
            --no-update-modtime \
            --no-traverse"
          set +e
          rclone copy "r2src:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            ${COMMON_ARGS} --files-from "$SHARD" 2>&1 | tee "/tmp/$(basename "$SHARD").log"
          RC=${PIPESTATUS[0]}
          set -e
          exit $RC
          EOS
          chmod +x /tmp/migrate_one.sh

      - name: Migrate shards in parallel（受控并发）
        run: |
          set -euxo pipefail
          ls -1 /tmp/pass/chunk_* > /tmp/pass.list
          parallel -j "${MAX_PAR}" --halt now,fail=1 /tmp/migrate_one.sh {} :::: /tmp/pass.list

      - name: Stats (size-only preview)
        if: always()
        run: |
          {
            echo "### PASS ${PASS_INDEX}/${PASS_COUNT} 统计"
            echo "\`\`\`json"
            echo "源："; rclone size "r2src:${SRC_BUCKET}" --json | jq .
            echo "目："; rclone size "r2dst:${DST_BUCKET}" --json | jq .
            echo "\`\`\`"
          } >> $GITHUB_STEP_SUMMARY

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-pass-${{ inputs.PASS_INDEX }}
          path: /tmp/*.log
          retention-days: 7

      - name: Compute next pass index
        id: calc
        run: |
          set -euo pipefail
          CUR="${{ inputs.PASS_INDEX }}"
          TOTAL="${{ inputs.PASS_COUNT }}"
          NEXT=$(( CUR + 1 ))
          if [ "$NEXT" -lt "$TOTAL" ]; then
            echo "has_next=true"  >> "$GITHUB_OUTPUT"
            echo "next_index=$NEXT" >> "$GITHUB_OUTPUT"
          else
            echo "has_next=false" >> "$GITHUB_OUTPUT"
            echo "next_index=$NEXT" >> "$GITHUB_OUTPUT"
          fi

      - name: Trigger next PASS（自动接力）
        if: ${{ steps.calc.outputs.has_next == 'true' }}
        uses: actions/github-script@v7
        env:
          NEXT_INDEX:   ${{ steps.calc.outputs.next_index }}
          SRC_BUCKET:   ${{ env.SRC_BUCKET }}
          DST_BUCKET:   ${{ env.DST_BUCKET }}
          MODE:         ${{ env.MODE }}
          SHARDS:       ${{ env.SHARDS }}
          PASS_COUNT:   ${{ env.PASS_COUNT }}
          MAX_PAR:      ${{ env.MAX_PAR }}
          TRANSFERS:    ${{ env.TRANSFERS }}
          CHECKERS:     ${{ env.CHECKERS }}
          CHUNK_SIZE:   ${{ env.CHUNK_SIZE }}
        with:
          script: |
            const next = process.env.NEXT_INDEX;
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: context.workflow,
              ref: context.ref.replace('refs/heads/',''),
              inputs: {
                SRC_BUCKET:   process.env.SRC_BUCKET,
                DST_BUCKET:   process.env.DST_BUCKET,
                MODE:         process.env.MODE,
                SHARDS:       process.env.SHARDS,
                PASS_INDEX:   next,
                PASS_COUNT:   process.env.PASS_COUNT,
                MAX_PARALLEL: process.env.MAX_PAR,
                TRANSFERS:    process.env.TRANSFERS,
                CHECKERS:     process.env.CHECKERS,
                CHUNK_SIZE:   process.env.CHUNK_SIZE
              }
            });
            core.notice(`已触发下一波 PASS=${next}/${process.env.PASS_COUNT}`);

  # 可选：总迁移完成后手动跑一次（或另开workflow）做真正镜像删除
  final_sync:
    if: ${{ false }}
    runs-on: ubuntu-latest
    timeout-minutes: 360
    steps:
      - name: Install rclone & jq
        run: |
          set -euxo pipefail
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq
          rclone version

      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT:  ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT:   ${{ secrets.R2DST_ENDPOINT }}

      - name: Mirror delete extras (size-only)
        run: |
          set -euxo pipefail
          rclone sync "r2src:${{ inputs.SRC_BUCKET }}" "r2dst:${{ inputs.DST_BUCKET }}" \
            --fast-list --size-only --delete-after --checkers 256 --transfers 64 \
            --log-level INFO --progress

      - name: Post-check summary
        if: always()
        run: |
          {
            echo "### Final Sync 统计"
            echo "\`\`\`json"
            echo "源："; rclone size "r2src:${{ inputs.SRC_BUCKET }}" --json | jq .
            echo "目："; rclone size "r2dst:${{ inputs.DST_BUCKET }}" --json | jq .
            echo "\`\`\`"
          } >> $GITHUB_STEP_SUMMARY

      - name: Upload final-sync logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: final-sync-logs
          path: /tmp/*.log
          retention-days: 7
