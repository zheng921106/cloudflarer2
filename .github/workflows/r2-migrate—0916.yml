name: R2 Migrate (glob-sharded, rclone)

on:
  workflow_dispatch: {}          # 手动触发
  # 如需定时续跑，取消注释（UTC）
  # schedule:
  #   - cron: "0 * * * *"        # 每整点跑一次

env:
  # 桶名（按需改）
  SRC_BUCKET: yasyadong001
  DST_BUCKET: yas003

  # 速率/并发/稳定性（小文件友好，先稳后快）
  MIN_AGE: "60s"                 # 只搬 ≥60s 的稳定文件
  TRANSFERS: "4"
  CHECKERS: "8"
  UPLOAD_CONC: "4"
  BW_LIMIT: "0"                  # 不限带宽；如遇拥塞可设 "10M"
  TPS_LIMIT: "12"                # 如遇 429 降低
  CHUNK_SIZE: "32M"
  COPY_CUTOFF: "256M"
  MAX_RETRIES: "8"
  LOW_LEVEL_RETRIES: "20"

jobs:
  migrate:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        shard:
          - "0/**"
          - "1/**"
          - "2/**"
          - "3/**"
          - "4/**"
          - "5/**"
          - "6/**"
          - "7/**"
          - "8/**"
          - "9/**"
          - "a/**"
          - "b/**"
          - "c/**"
          - "d/**"
          - "e/**"
          - "f/**"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Configure rclone remotes (Cloudflare R2 → R2)
        env:
          # 源 R2（push）
          R2PUSH_ACCESS_KEY:  ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY:  ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT:    ${{ secrets.R2PUSH_ENDPOINT }}   # 例：https://44a6...r2.cloudflarestorage.com
          # 目的 R2（dst）
          R2DST_ACCESS_KEY:   ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY:   ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT:     ${{ secrets.R2DST_ENDPOINT }}     # 例：https://cff8...r2.cloudflarestorage.com
        run: |
          set -euo pipefail
          rclone config create r2push s3 \
            provider=Other env_auth=false \
            access_key_id="${R2PUSH_ACCESS_KEY}" secret_access_key="${R2PUSH_SECRET_KEY}" \
            region=auto endpoint="${R2PUSH_ENDPOINT}" \
            s3-chunk-size="${CHUNK_SIZE}" s3-upload-concurrency="${UPLOAD_CONC}"

          rclone config create r2dst s3 \
            provider=Other env_auth=false \
            access_key_id="${R2DST_ACCESS_KEY}" secret_access_key="${R2DST_SECRET_KEY}" \
            region=auto endpoint="${R2DST_ENDPOINT}" \
            s3-chunk-size="${CHUNK_SIZE}" s3-upload-concurrency="${UPLOAD_CONC}"

          rclone listremotes

      - name: Copy (glob shard)
        run: |
          set -euo pipefail
          mkdir -p logs
          SAFE_TAG="${{ matrix.shard//\//_ }}"
          LOG_FILE="logs/r2_copy_${SAFE_TAG}_$(date -u +%F_%H-%M-%S).log"

          echo "[$(date -u '+%F %T')] Copy with include glob: ${{ matrix.shard }}" | tee -a "$LOG_FILE"

          rclone copy "r2push:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            --fast-list --metadata --size-only \
            --min-age "${MIN_AGE}" \
            --include "${{ matrix.shard }}" --exclude="**" \
            --transfers "${TRANSFERS}" --checkers "${CHECKERS}" \
            --bwlimit "${BW_LIMIT}" --tpslimit "${TPS_LIMIT}" \
            --s3-chunk-size "${CHUNK_SIZE}" --s3-copy-cutoff "${COPY_CUTOFF}" \
            --retries "${MAX_RETRIES}" --low-level-retries "${LOW_LEVEL_RETRIES}" \
            --progress 2>&1 | tee -a "$LOG_FILE"

          echo "[$(date -u '+%F %T')] Shard done." | tee -a "$LOG_FILE"

      - name: Verify (size-only, one-way)
        if: always()
        run: |
          set -euo pipefail
          mkdir -p logs
          SAFE_TAG="${{ matrix.shard//\//_ }}"
          LOG_FILE="logs/r2_check_${SAFE_TAG}_$(date -u +%F_%H-%M-%S).log"

          rclone check "r2push:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            --one-way --size-only --checkers $((CHECKERS*2)) \
            --include "${{ matrix.shard }}" --exclude="**" \
            --progress 2>&1 | tee -a "$LOG_FILE" || true

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: r2-migrate-logs-${{ matrix.shard }}
          path: logs/
          retention-days: 7
