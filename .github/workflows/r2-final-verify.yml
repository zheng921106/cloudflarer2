name: R2 Final Verify (glob-sharded, checksum)

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "30 3 * * *"   # 可选：每日 03:30 收尾校验（UTC）

env:
  SRC_BUCKET: yasyadong001
  DST_BUCKET: yas004

  TRANSFERS: "4"
  CHECKERS: "16"
  UPLOAD_CONC: "4"
  BW_LIMIT: "0"
  TPS_LIMIT: "12"
  CHUNK_SIZE: "32M"
  COPY_CUTOFF: "256M"
  MAX_RETRIES: "8"
  LOW_LEVEL_RETRIES: "20"

jobs:
  verify:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        shard: [ "0","1","2","3","4","5","6","7","8","9","A","B","C","D","E","F" ]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Configure rclone remotes (Cloudflare R2 → R2)
        env:
          R2PUSH_ACCESS_KEY:  ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY:  ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT:    ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY:   ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY:   ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT:     ${{ secrets.R2DST_ENDPOINT }}
        run: |
          set -euo pipefail
          rclone config create r2push s3 \
            provider=Other env_auth=false \
            access_key_id="${R2PUSH_ACCESS_KEY}" secret_access_key="${R2PUSH_SECRET_KEY}" \
            region=auto endpoint="${R2PUSH_ENDPOINT}" \
            s3-chunk-size="${CHUNK_SIZE}" s3-upload-concurrency="${UPLOAD_CONC}"

          rclone config create r2dst s3 \
            provider=Other env_auth=false \
            access_key_id="${R2DST_ACCESS_KEY}" secret_access_key="${R2DST_SECRET_KEY}" \
            region=auto endpoint="${R2DST_ENDPOINT}" \
            s3-chunk-size="${CHUNK_SIZE}" s3-upload-concurrency="${UPLOAD_CONC}"

          rclone listremotes

      - name: Ensure destination bucket exists
        run: |
          set -euo pipefail
          if ! rclone lsd "r2dst:" | grep -qE "\s${DST_BUCKET}\s*$"; then
            echo "Creating destination bucket: ${DST_BUCKET}"
            rclone mkdir "r2dst:${DST_BUCKET}"
          else
            echo "Destination bucket ${DST_BUCKET} exists."
          fi

      - name: Build filter file for this shard
        id: buildfilter
        run: |
          set -euo pipefail
          mkdir -p filters
          FILTER="filters/${{ matrix.shard }}.filter"
          : > "$FILTER"

          # 列出顶层“目录”（伪目录），只取第一层
          rclone lsf "r2push:${SRC_BUCKET}" --dirs-only --max-depth 1 > all_dirs.txt

          # 选出：首字符匹配当前 shard，且目录名是 32 位十六进制（不区分大小写）
          # 目录名形如 ABCD...（32位）后面跟一个斜杠
          awk -v sh="${{ matrix.shard }}" '
            BEGIN{ IGNORECASE=1 }
            /^[0-9A-F]{32}\/$/ {
              dir=$0
              first=substr(dir,1,1)
              if (tolower(first)==tolower(sh)) print dir
            }
          ' all_dirs.txt > matched_dirs.txt

          echo "Matched dirs for shard ${{ matrix.shard }}:"
          cat matched_dirs.txt || true

          # 生成 rclone 过滤规则：+ <dir>** ；最后一行 - ** 作为兜底排除
          if [ -s matched_dirs.txt ]; then
            while IFS= read -r d; do
              printf "+ %s**\n" "$d"
            done < matched_dirs.txt > "$FILTER"
            echo "- **" >> "$FILTER"
            echo "file=$FILTER" >> "$GITHUB_OUTPUT"
            echo "empty=false" >> "$GITHUB_OUTPUT"
          else
            # 本分片没有匹配
            echo "# empty shard" > "$FILTER"
            echo "file=$FILTER" >> "$GITHUB_OUTPUT"
            echo "empty=true" >> "$GITHUB_OUTPUT"
          fi

      - name: Repair copy (checksum)
        if: steps.buildfilter.outputs.empty == 'false'
        run: |
          set -euo pipefail
          mkdir -p logs
          LOG_FILE="logs/r2_repair_${{ matrix.shard }}_$(date -u +%F_%H-%M-%S).log"

          echo "[修复复制] shard=${{ matrix.shard }} 使用过滤规则：${{ steps.buildfilter.outputs.file }}" | tee -a "$LOG_FILE"

          rclone copy "r2push:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            --fast-list --metadata --checksum \
            --filter-from "${{ steps.buildfilter.outputs.file }}" \
            --transfers "${TRANSFERS}" --checkers "${CHECKERS}" \
            --bwlimit "${BW_LIMIT}" --tpslimit "${TPS_LIMIT}" \
            --s3-chunk-size "${CHUNK_SIZE}" --s3-copy-cutoff "${COPY_CUTOFF}" \
            --retries "${MAX_RETRIES}" --low-level-retries "${LOW_LEVEL_RETRIES}" \
            --progress 2>&1 | tee -a "$LOG_FILE"

      - name: Strict verify (checksum)
        if: steps.buildfilter.outputs.empty == 'false'
        run: |
          set -euo pipefail
          mkdir -p logs
          LOG_FILE="logs/r2_verify_${{ matrix.shard }}_$(date -u +%F_%H-%M-%S).log"

          echo "[严格校验] shard=${{ matrix.shard }} 使用过滤规则：${{ steps.buildfilter.outputs.file }}" | tee -a "$LOG_FILE"

          rclone check "r2push:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            --one-way --checksum --checkers "${CHECKERS}" \
            --filter-from "${{ steps.buildfilter.outputs.file }}" \
            --progress 2>&1 | tee -a "$LOG_FILE"

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: r2-final-verify-logs-${{ matrix.shard }}
          path: logs/
          retention-days: 14
