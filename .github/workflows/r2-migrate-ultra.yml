name: Cloudflare R2 Mega Migration (stream-shard + pass-chaining)

on:
  workflow_dispatch:
    inputs:
      SRC_BUCKET:   { description: "A桶（来源）", required: true, default: "yasyadong001", type: string }
      DST_BUCKET:   { description: "B桶（目标）", required: true, default: "yas012", type: string }
      MODE:         { description: "模式：copy(默认) / sync(最后做一次清理)", required: true, default: "copy", type: choice, options: ["copy","sync"] }
      SHARDS:       { description: "总分片数（建议 1024~4096）", required: true, default: "2048", type: string }
      PASS_INDEX:   { description: "当前波次（从0开始）", required: true, default: "0", type: string }
      PASS_COUNT:   { description: "总波次数（越大每波越短）", required: true, default: "12", type: string }
      MAX_PARALLEL: { description: "每波并行分片数（保守：4~8）", required: true, default: "6", type: string }
      TRANSFERS:    { description: "rclone传输并发（小文件建议 64~128）", required: true, default: "96", type: string }
      CHECKERS:     { description: "rclone校验并发", required: true, default: "256", type: string }
      CHUNK_SIZE:   { description: "S3分块阈值/大小（小文件别太低）", required: true, default: "256M", type: string }

jobs:
  migrate:
    name: PASS ${{ inputs.PASS_INDEX }}/${{ inputs.PASS_COUNT }} • ${{ inputs.SRC_BUCKET }} → ${{ inputs.DST_BUCKET }}
    runs-on: ubuntu-latest
    timeout-minutes: 350
    permissions:
      actions: write   # 允许触发下一波
    env:
      SRC_BUCKET:   ${{ inputs.SRC_BUCKET }}
      DST_BUCKET:   ${{ inputs.DST_BUCKET }}
      MODE:         ${{ inputs.MODE }}
      SHARDS:       ${{ inputs.SHARDS }}
      PASS_INDEX:   ${{ inputs.PASS_INDEX }}
      PASS_COUNT:   ${{ inputs.PASS_COUNT }}
      MAX_PAR:      ${{ inputs.MAX_PARALLEL }}
      TRANSFERS:    ${{ inputs.TRANSFERS }}
      CHECKERS:     ${{ inputs.CHECKERS }}
      CHUNK_SIZE:   ${{ inputs.CHUNK_SIZE }}

    steps:
      - name: Mask secrets
        run: |
          echo "::add-mask::${{ secrets.R2PUSH_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_ENDPOINT }}"
          echo "::add-mask::${{ secrets.R2DST_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_ENDPOINT }}"

      - name: Install rclone, jq, GNU parallel, mawk
        run: |
          set -euxo pipefail
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq parallel mawk
          rclone version
          parallel --version
          mawk -W version || true

      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT:  ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT:   ${{ secrets.R2DST_ENDPOINT }}

      - name: Preflight checks（端点/桶）
        run: |
          set -euxo pipefail
          rclone lsd "r2src:${SRC_BUCKET}"
          rclone mkdir "r2dst:${DST_BUCKET}" || true
          rclone lsd "r2dst:${DST_BUCKET}"

      - name: Stream-shard only shards for THIS PASS（无全量落地清单）
        run: |
          set -euxo pipefail
          mkdir -p /tmp/pass
          # 说明：
          # 1) 我们不生成 all_keys.txt；直接流式列举 → 通过哈希分桶 → 仅写入本 PASS 需要的 shard 文件
          # 2) 自定义 mawk 哈希（djb2，快且分布均匀），内存常量级
          cat > /tmp/stream_shard.awk <<'AWK'
          BEGIN {
            pc = ENVIRON["PASS_COUNT"] + 0
            pi = ENVIRON["PASS_INDEX"] + 0
            shards = ENVIRON["SHARDS"] + 0
          }
          function djb2(s,   h,i,c) {
            h = 5381
            for (i=1; i<=length(s); i++) {
              c = ord(substr(s,i,1))
              h = ((h * 33) + c) % 2147483647
            }
            return h
          }
          function ord(ch) { return index("\0\1\2\3\4\5\6\7\10\11\12\13\14\15\16\17\20\21\22\23\24\25\26\27\30\31\32\33\34\35\36\37 !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~", ch)-1 }
          {
            # rclone lsjson 输出的是 JSON 行，这里我们在外层用 jq 提取 Path，这里只接纯路径
            key = $0
            h = djb2(key)
            sid = h % shards
            # 只落地属于本 PASS 的分片（sid % PASS_COUNT == PASS_INDEX）
            if ((sid % pc) == pi) {
              fn = sprintf("/tmp/pass/chunk_%04d", sid)
              print key >> fn
            }
          }
          AWK

          # 用 lsjson → jq -r .Path 保证路径正确，避免 --fast-list 内存爆炸
          # 注：--fast-list 可能更省 API 但会吃内存，这里不用；采用分页默认模式，稳定优先
          rclone lsjson "r2src:${SRC_BUCKET}" -R --files-only --no-mimetype --no-modtime \
            | jq -r '.[].Path' \
            | mawk -f /tmp/stream_shard.awk

          # 如果本 PASS 没键，直接结束（可能数据偏少或 PASS_COUNT 偏大）
          cnt=$(ls -1 /tmp/pass/chunk_* 2>/dev/null | wc -l || echo 0)
          echo "This PASS shards: ${cnt}"
          if [ "$cnt" -eq 0 ]; then
            echo "::notice::本 PASS 没有需要处理的分片。"
            exit 0
          fi

      - name: Create migrate script（小文件优化参数）
        run: |
          cat > /tmp/migrate_one.sh <<'EOS'
          #!/usr/bin/env bash
          set -euo pipefail
          SHARD="$1"
          COMMON_ARGS="--progress --log-level INFO --stats=60s --stats-one-line \
            --transfers=${TRANSFERS} --checkers=${CHECKERS} \
            --s3-upload-cutoff=${CHUNK_SIZE} --s3-chunk-size=${CHUNK_SIZE} \
            --s3-upload-concurrency=4 \
            --retries=20 --retries-sleep=20s --low-level-retries=60 \
            --timeout=300s --contimeout=120s \
            --buffer-size=32M --use-mmap \
            --multi-thread-streams=4 --multi-thread-cutoff=16M \
            --s3-no-check-bucket \
            --ignore-existing \
            --no-update-modtime \
            --no-traverse"
          # 说明：
          # - 小文件为主：提高 --transfers；减小 buffer-size；限制 per-file 并发，避免过多 FD/内存
          # - --no-traverse 对 S3→S3 能减少目的端列举开销（我们用 files-from 精准定位）
          # - 这里统一做 copy；若 MODE=sync，最后单独做一次全量 sync 删除冗余

          set +e
          rclone copy "r2src:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            ${COMMON_ARGS} --files-from "$SHARD" 2>&1 | tee "/tmp/$(basename "$SHARD").log"
          RC=${PIPESTATUS[0]}
          set -e
          exit $RC
          EOS
          chmod +x /tmp/migrate_one.sh

      - name: Migrate shards in parallel（受控并发）
        run: |
          set -euxo pipefail
          ls -1 /tmp/pass/chunk_* > /tmp/pass.list
          parallel -j "${MAX_PAR}" --halt now,fail=1 /tmp/migrate_one.sh {} :::: /tmp/pass.list

      - name: Stats (size-only preview)
        if: always()
        run: |
          {
            echo "### PASS ${PASS_INDEX}/${PASS_COUNT} 统计"
            echo "\`\`\`json"
            echo "源："; rclone size "r2src:${SRC_BUCKET}" --json | jq .
            echo "目："; rclone size "r2dst:${DST_BUCKET}" --json | jq .
            echo "\`\`\`"
          } >> $GITHUB_STEP_SUMMARY

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-pass-${{ inputs.PASS_INDEX }}
          path: /tmp/*.log
          retention-days: 7

      - name: Trigger next PASS（自动接力）
        if: ${{ (fromJSON(inputs.PASS_INDEX) + 1) < fromJSON(inputs.PASS_COUNT) }}
        uses: actions/github-script@v7
        with:
          script: |
            const next = (parseInt(core.getInput('PASS_INDEX')) + 1).toString();
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: context.workflow,
              ref: context.ref.replace('refs/heads/',''),
              inputs: {
                SRC_BUCKET:   process.env.SRC_BUCKET,
                DST_BUCKET:   process.env.DST_BUCKET,
                MODE:         process.env.MODE,
                SHARDS:       process.env.SHARDS,
                PASS_INDEX:   next,
                PASS_COUNT:   process.env.PASS_COUNT,
                MAX_PARALLEL: process.env.MAX_PAR,
                TRANSFERS:    process.env.TRANSFERS,
                CHECKERS:     process.env.CHECKERS,
                CHUNK_SIZE:   process.env.CHUNK_SIZE
              }
            });
            core.notice(`已触发下一波 PASS=${next}/${process.env.PASS_COUNT}`);

  # 可选：总迁移完成后手动跑一次（或另开workflow）做真正镜像删除
  final_sync:
    if: ${{ false }}   # 需要时改为 true 或单独开工作流
    runs-on: ubuntu-latest
    steps:
      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT:  ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT:   ${{ secrets.R2DST_ENDPOINT }}
      - name: Mirror delete extras (size-only)
        run: |
          rclone sync "r2src:${{ inputs.SRC_BUCKET }}" "r2dst:${{ inputs.DST_BUCKET }}" \
            --fast-list --size-only --delete-after --checkers 256 --transfers 64 \
            --log-level INFO --progress
