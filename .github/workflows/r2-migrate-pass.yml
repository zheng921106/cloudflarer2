name: Cloudflare R2 Bucket Migration (rclone) - Pass-Limited

on:
  workflow_dispatch:
    inputs:
      SRC_BUCKET: { description: "A桶（来源）", required: true, default: "yasyadong001", type: string }
      DST_BUCKET: { description: "B桶（目标）", required: true, default: "yas012", type: string }
      MODE:       { description: "迁移模式", required: true, default: "copy", type: choice, options: ["copy","sync"] }
      TRANSFERS:  { description: "并发传输数", required: true, default: "64", type: string }
      CHECKERS:   { description: "并发校验数", required: true, default: "128", type: string }
      CHUNK_SIZE: { description: "S3分块大小", required: true, default: "256M", type: string }
      SHARDS:     { description: "总分片数", required: true, default: "200", type: string }
      MAX_PARALLEL: { description: "单波次并行分片数", required: true, default: "8", type: string }
      PASS_INDEX: { description: "当前波次序号（从0开始）", required: true, default: "0", type: string }
      PASS_COUNT: { description: "总波次数（建议 4~12）", required: true, default: "6", type: string }

jobs:
  migrate:
    name: PASS ${{ inputs.PASS_INDEX }}/${{ inputs.PASS_COUNT }}  •  ${{ inputs.SRC_BUCKET }} → ${{ inputs.DST_BUCKET }}
    runs-on: ubuntu-latest
    timeout-minutes: 350   # 严控单次运行小于6小时
    permissions:
      actions: write   # 允许后续自动触发下一波
    env:
      SRC_BUCKET:   ${{ inputs.SRC_BUCKET }}
      DST_BUCKET:   ${{ inputs.DST_BUCKET }}
      MODE:         ${{ inputs.MODE }}
      TRANSFERS:    ${{ inputs.TRANSFERS }}
      CHECKERS:     ${{ inputs.CHECKERS }}
      CHUNK_SIZE:   ${{ inputs.CHUNK_SIZE }}
      SHARDS:       ${{ inputs.SHARDS }}
      MAX_PAR:      ${{ inputs.MAX_PARALLEL }}
      PASS_INDEX:   ${{ inputs.PASS_INDEX }}
      PASS_COUNT:   ${{ inputs.PASS_COUNT }}

    steps:
      - name: Mask secrets
        run: |
          echo "::add-mask::${{ secrets.R2PUSH_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2PUSH_ENDPOINT }}"
          echo "::add-mask::${{ secrets.R2DST_ACCESS_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_SECRET_KEY }}"
          echo "::add-mask::${{ secrets.R2DST_ENDPOINT }}"

      - name: Install rclone, jq, GNU parallel
        run: |
          set -euxo pipefail
          curl -fsSL https://rclone.org/install.sh | sudo bash
          sudo apt-get update -y
          sudo apt-get install -y jq parallel
          rclone version

      - name: Prepare rclone.conf
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2src]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2PUSH_ACCESS_KEY}
          secret_access_key = ${R2PUSH_SECRET_KEY}
          endpoint = ${R2PUSH_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true

          [r2dst]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2DST_ACCESS_KEY}
          secret_access_key = ${R2DST_SECRET_KEY}
          endpoint = ${R2DST_ENDPOINT}
          region = auto
          force_path_style = true
          no_check_bucket = true
          EOF
        env:
          R2PUSH_ACCESS_KEY: ${{ secrets.R2PUSH_ACCESS_KEY }}
          R2PUSH_SECRET_KEY: ${{ secrets.R2PUSH_SECRET_KEY }}
          R2PUSH_ENDPOINT:  ${{ secrets.R2PUSH_ENDPOINT }}
          R2DST_ACCESS_KEY: ${{ secrets.R2DST_ACCESS_KEY }}
          R2DST_SECRET_KEY: ${{ secrets.R2DST_SECRET_KEY }}
          R2DST_ENDPOINT:   ${{ secrets.R2DST_ENDPOINT }}

      - name: Preflight checks
        run: |
          set -euxo pipefail
          rclone lsd "r2src:${SRC_BUCKET}"
          rclone mkdir "r2dst:${DST_BUCKET}" || true
          rclone lsd "r2dst:${DST_BUCKET}"

      - name: Build & shard all keys (once per workflow run)
        run: |
          set -euxo pipefail
          mkdir -p /tmp/plan/keylists
          if [ -s /tmp/plan/all_keys.txt ]; then
            echo "Reuse /tmp/plan/all_keys.txt"
          else
            rclone lsf "r2src:${SRC_BUCKET}" -R --files-only --fast-list > /tmp/plan/all_keys.txt
          fi
          total=$(wc -l < /tmp/plan/all_keys.txt)
          echo "Total objects: $total"
          if [ "$total" -eq 0 ]; then
            echo "::notice::源桶为空，直接完成。"; exit 0
          fi
          split -n l/${SHARDS} -d -a 4 /tmp/plan/all_keys.txt /tmp/plan/keylists/chunk_
          ls -1 /tmp/plan/keylists/chunk_* | wc -l

      - name: Select shards for this PASS
        id: pick
        run: |
          set -euxo pipefail
          ls -1 /tmp/plan/keylists/chunk_* | nl -ba > /tmp/all.idx
          # 选择 (idx-1) % PASS_COUNT == PASS_INDEX 的分片
          awk -v pc=${PASS_COUNT} -v pi=${PASS_INDEX} '{ if ((($1-1) % pc) == pi) print $2 }' /tmp/all.idx > /tmp/pass.list
          count=$(wc -l < /tmp/pass.list)
          echo "This PASS will process $count shards."
          echo "count=$count" >> $GITHUB_OUTPUT

      - name: Create migrate script
        run: |
          cat > /tmp/migrate_one.sh <<'EOS'
          #!/usr/bin/env bash
          set -euo pipefail
          SHARD="$1"
          COMMON_ARGS="--progress --log-level INFO --stats=60s --stats-one-line \
            --transfers=${TRANSFERS} --checkers=${CHECKERS} \
            --s3-upload-cutoff=${CHUNK_SIZE} --s3-chunk-size=${CHUNK_SIZE} \
            --retries=10 --retries-sleep=30s --low-level-retries=30 \
            --timeout=300s --contimeout=120s \
            --buffer-size=512M --use-mmap \
            --multi-thread-streams=8 --multi-thread-cutoff=64M \
            --fast-list --s3-no-check-bucket \
            --ignore-existing"
          set +e
          rclone copy "r2src:${SRC_BUCKET}" "r2dst:${DST_BUCKET}" \
            ${COMMON_ARGS} --files-from "$SHARD" 2>&1 | tee "/tmp/$(basename "$SHARD").log"
          RC=${PIPESTATUS[0]}
          set -e
          exit $RC
          EOS
          chmod +x /tmp/migrate_one.sh

      - name: Run this PASS in parallel
        if: ${{ steps.pick.outputs.count != '0' }}
        run: |
          set -euxo pipefail
          parallel -j "${MAX_PAR}" --halt now,fail=1 /tmp/migrate_one.sh {} :::: /tmp/pass.list

      - name: Quick stats
        if: always()
        run: |
          {
            echo "### PASS ${PASS_INDEX}/${PASS_COUNT} 统计"
            echo "\`\`\`json"
            echo "源："; rclone size "r2src:${SRC_BUCKET}" --json | jq .
            echo "目："; rclone size "r2dst:${DST_BUCKET}" --json | jq .
            echo "\`\`\`"
          } >> $GITHUB_S_
